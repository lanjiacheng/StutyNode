hadoop安装：
--------------------------------
1.独立模式		//独立模式下，没有守护进程，没有分布式文件系统，用的是系统的文件系统
	1.下载
	2.tar开hadoop软件包文件
	3.移动到/soft/
	4.配置环境变量
		[/etc/environment]
		...
		HADOOP_HOME=/soft/hadoop
		PATH=...:/soft/hadoop/bin:/soft/hadoop/sbin
	5.测试安装是否成功
		$>hadoop version
说明：没有守护进程，所有程序运行在同一JVM中，利于test和debug



2.伪分布模式
-------------------------------
	配置hadoop:
	${hadoop_home}/etc/hadoop/*-site.xml
	[core-site.xml]
	<configuration>
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://localhost/</value>
		</property>
	</configuration>

	[hdfs-site.xml]
		dfs.replication=1

	[mapred-site.xml]
		mapreduce.framework.name=yarn

	[yarn-site.xml]
		yarn.resourcemanager.hostname=localhost
		yarn.nodemanager.aux-services=mapreduce_shuffie

	[配置SSH]
		sudo apt-cache search ssh | grep ssh

		1.sudo apt-get install ssh		//安装sshd（服务端）和ssh（客户端）和相关软件ssh-keygen
		2.ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa	//生成秘钥和公钥
		3.cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys	//把秘钥发到需要登录本机器的本账户的机器上（本机），追加公钥到对方的认证库中
		4.测试ssh到localhost
			$>ssh localhost
scp root@s200:/etc/hosts root@s300:/etc/	//复制 s200中的hosts到s300中的指定目录下去
scp hosts root@s200:/etc/			//复制本机当前目录下的hosts到s200的指定目录下
ssh s200 rm -rf /soft/hadoop/etc/hadoop_cluster	//登进s200后执行后面操作


3.完全分布式模式


4.格式化hdfs文件系统
	$>hdfs namenode -format		//hadoop/bin/hdfs
5.启动hdfs和yarn守护进程
	$>start-dfs.sh		//stop...关闭
	$start-yarn.sh		//hadoop/sbin/stop
6.配置目录的指定方式
	a.默认方式
		${hadoop_home}/etc/hadoop/*.xml
	b.通过启动参数指定配置目录
		$>start-dfs.sh --config /soft/hadoop/etc/hadoop [command]
	c.通过设置环境变量HADOOP_CONF_DIR
		$>export HADOOP_CONF_DIR=/soft/hadoop/etc/hadoop
	d.通过在/soft/hadopp.../etc/下创建指定配置文件夹的符号链接来指定
		$>ln -s hadoop_local hadoop
		//如果事先设置了环境变量，这样删除：export HADOOP_CONF_DIR=
7.目录操作
	$>hadoop fs -mkdir -p /user/ubuntu	//创建目录 -p  多级目录
	$>hadoop fs -ls /			//显示根目录
	$>hadoop fs -ls -R /			//递归显示目录结构
	$>hadoop fs -lsr /			//同上
	$>hadoop fs				//查看帮助
	$>hadoop fs -help cmd			//查看指定命令的帮助
	$>hadoop fs -put hello.txt /user/ubuntu/	//放文件到hdfs的指定目录
	$>hadoop fs -ls file:/			//列出本地系统的文件系统
hdfs对应文件目录：/tmp/hadoop-ljc/dfs/
存放文件到hdfs，以块形式存放，物理目录如下：
	/tmp/hadoop-ljc/dfs/data/current/BP.../current/finalized/subdir0/subdir0/blk
8.查看log文件（日志文件）
	$>${hadoop_home}/logs
9.通过webui查看日志或集群情况
	http://namenode:50070/		//namenode
	http://localhost:8088/		//resourceManager，只能本机访问，hadoop集群信息
	http://hs:19888/		//historyServer
	hdfs://namenode:8020/		//name roc(remote procedure call，远程调用)
	http://ubuntu-02:8042/logs	//查看数据节点日志


搭建完全分布式hadoop集群
1.安装java
2.
3.安装hadoop
4.配置ssh
5.格式化
6.启动守护进程
7.创建目录
8.配置hadoop
	a.克隆虚拟机4台
	b.
	c.配置hadoop配置文件
		${hadoop_home}/etc/hadoop/*-site.xml
	[core-site.xml]
	<configuration>
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://s100/</value>		//s100是namenode节点的主机名，需配置
		</property>

		//修改默认的hadoop临时目录
		hadoop.tmp.dir=/home/ubuntu/hadoop
	</configuration>

	[hdfs-site.xml]
		dfs.replication=3		//配置数据节点副本数
		dfs.namenode.secondary.http-address=ubuntu-05:50090	//配置默认辅助名称结点主机地址

	[mapred-site.xml]
		mapreduce.framework.name=yarn

	[yarn-site.xml]
		yarn.resourcemanager.hostname=namenode
		yarn.nodemanager.aux-services=mapreduce_shuffle

	[slaves]
	s200
	s300
	s400
	

	[/etc/hosts]	//配置域名解析

	d.远程复制/etc/hosts到每台远程主机
		$>scp hosts root@s200:/etc/
	e.配公钥，将本机（namenode结点）的ssh公钥发到每台机子上，然后通过公钥生成认证文件
		$>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa	//生成秘钥和公钥
		$>scp id_rsa.pub ubuntu@S200:~/.ssh/		//远程发公钥
		$>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys	//在s200通过公钥生成认证文件
		//以此类推，配完3台datanode结点
	f.将配好的${hadoop}/etc/hadoop_cluster文件scp到每台机上
9.完全布模式下，一般使用--config来启动进程，启动进程前，先格式化，格式化指令：
		hdfs --config /soft/hadoop/etc/hadoop_cluster namenode -format
		hdfs --config /soft/hadoop/etc/hadoop_cluster getconf -secondarynamenodes	//获取配置信息



-----------------------------------
jps		//查看java进程
/etc/hosts	//域名解析文件